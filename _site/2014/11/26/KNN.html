<p>﻿—
layout: post
category : 算法
tagline: “Learn”
tags : [机器学习]
—
#K近邻法（KNN）
K近邻法(K-nearest neighbor,K-NN)是一种基本分类和回归方法，最近在《统计学习方法》上学了分类问题中的K近邻。
同时给出了K近邻的一种实现方法，KD-TREE。</p>

<h2 id="section">一、距离度量</h2>

<h3 id="minkowski">Minkowski距离</h3>

<p><script type="math/tex">设特征空间\chi是n维实数向量空间R^n,x_i,x_j\in\chi,</script>
<script type="math/tex">x_i=(x_i^{(1)},x_i^{(2)},x_i^{(3)},……,x_i^{(n)})^T,</script>
<script type="math/tex">x_j=(x_j^{(1)},x_j^{(2)},x_j^{(3)},……,x_j^{(n)})^T,</script>
<script type="math/tex">那么x_i,x_j的L_p距离定义为:</script>
<script type="math/tex">L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^{p})^{1/p}</script></p>

<h4 id="p1manhattan-distance">当P=1时,称为曼哈顿距离(Manhattan distance):</h4>
<p><script type="math/tex">L_1(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|)</script></p>

<h4 id="p2euclidean-distance">当P=2时，称为欧式距离(Euclidean distance):</h4>
<p><script type="math/tex">L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{1/2}</script></p>

<h4 id="pinfchebyshev-distance">当P=inf时，称为切比雪夫距离(Chebyshev distance):</h4>
<p><script type="math/tex">L_{\infty}(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|</script></p>

<p>不同的距离度量，选取的最近邻点也是不同的。</p>

<h2 id="k">二、K值的选择</h2>

<p>k值选择对结果影响非常大，选择较大的k值相当于用大的邻域中的训练实例来进行预测，减少了学习的估计误差(estimation error)，但是学习的近似误差(approximation error)会增大。这是与实例较远的（不相似）的训练实例也会对结果产生影响。k值增大意味着整体模型变得简单。
当K=N时，此时无论输入什么，都将简单的被预测为训练实例中最多的类，显然是不科学的。
在应用中，一般使用交叉验证的方法选取K值，简单来说就是把训练实例，一部分训练，一部分验证。</p>

<h2 id="kkdtree">三、K近邻实现：KD_TREE</h2>

<h3 id="section-1">1.构造</h3>

<script type="math/tex; mode=display">训练实例T=(x_1,x_2,x_3…x_N)</script>

<script type="math/tex; mode=display">x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T, i = 1 , 2 , 3 …，N</script>

<p>（一）选择<script type="math/tex">x^{(1)}</script>为当前坐标轴，对所有训练实例选出<script type="math/tex">x^{(1)}</script>的中位数，通过该点在垂直于<script type="math/tex">x^{(1)}</script>的
坐标轴上切分，将平面分为两个子区域。
生成之后，左子结点的<script type="math/tex">x^{(1)}</script>小于切分点的<script type="math/tex">x^{(1)}</script>,右子结点的<script type="math/tex">x^{(1)}</script>大于切分点的<script type="math/tex">x^{(1)}</script>。</p>

<p>（二）重复该过程，假设当前点的深度为j,那么选择<script type="math/tex">l=(j\%k)+1</script>,即<script type="math/tex">x^{(l)}</script>的中位数为切分点，
垂直<script type="math/tex">x^{(l)}</script>轴进行切分。</p>

<p>（三）直到没有实例可以切分，此时KD树划分完成</p>

<h3 id="section-2">2.搜索</h3>

<p>（一）从根节点出发，如果x的当前小于切分点的坐标，则递归到左子结点，否则递归到右子结点。知道为叶结点为止。
（二）已此叶结点为“当前最近点”，不断回退。
（三）用当前节点来更新“当前最近点”。
（四）“当前最近点”一定是当前节点一个子节点对应的区域，检查该子节点对于的父节点的另一区域是否有更近的点，即，检查另一子节点对应的区域是否与以目标点为球心，“当前最近点”为半径的超球体相交。若相交，搜索另一区域，否则继续回退。（即如果当前划分域为<script type="math/tex">x^{(l)}</script>，并且该域中位数的点为<script type="math/tex">x_k</script>,那么即判断该超球体是否与<script type="math/tex">x^{(l)}=x_k^{(l)}</script>相交）
（五）回退到根节点，结束搜索。</p>

<h3 id="section-3">3.实现</h3>

<p>见我另一篇博客<a href="http://kdqzzxxcc.github.io/2014/11/03/KD_Tree%E5%AD%A6%E4%B9%A0.html">kdt</a></p>
