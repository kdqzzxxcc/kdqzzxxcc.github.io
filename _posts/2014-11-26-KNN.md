---
layout: post
category : 算法
tagline: "Learn"
tags : [机器学习]
---
#K近邻法（KNN）
K近邻法(K-nearest neighbor,K-NN)是一种基本分类和回归方法，最近在《统计学习方法》上学了分类问题中的K近邻。
同时给出了K近邻的一种实现方法，KD-TREE。
	
##一、距离度量

###Minkowski距离

$$设特征空间\chi是n维实数向量空间R^n,x_i,x_j\in\chi,$$
$$x_i=(x_i^{(1)},x_i^{(2)},x_i^{(3)},……,x_i^{(n)})^T,$$
$$x_j=(x_j^{(1)},x_j^{(2)},x_j^{(3)},……,x_j^{(n)})^T,$$
$$那么x_i,x_j的L_p距离定义为:$$
$$L_p(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^{p})^{1/p}$$

####当P=1时,称为曼哈顿距离(Manhattan distance):
$$L_1(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|)$$

####当P=2时，称为欧式距离(Euclidean distance):
$$L_2(x_i,x_j)=(\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{1/2}$$

####当P=inf时，称为切比雪夫距离(Chebyshev distance):
$$L_{\infty}(x_i,x_j)=\max_l|x_i^{(l)}-x_j^{(l)}|$$

不同的距离度量，选取的最近邻点也是不同的。

##二、K值的选择

k值选择对结果影响非常大，选择较大的k值相当于用大的邻域中的训练实例来进行预测，减少了学习的估计误差(estimation error)，但是学习的近似误差(approximation error)会增大。这是与实例较远的（不相似）的训练实例也会对结果产生影响。k值增大意味着整体模型变得简单。
当K=N时，此时无论输入什么，都将简单的被预测为训练实例中最多的类，显然是不科学的。
在应用中，一般使用交叉验证的方法选取K值，简单来说就是把训练实例，一部分训练，一部分验证。
	
##三、K近邻实现：KD_TREE

###1.构造

$$训练实例T=(x_1,x_2,x_3…x_N)$$

$$x_i=(x_i^{(1)},x_i^{(2)},…,x_i^{(k)})^T, i = 1 , 2 , 3 …，N$$

（一）选择$$x^{(1)}$$为当前坐标轴，对所有训练实例选出$$x^{(1)}$$的中位数，通过该点在垂直于$$x^{(1)}$$的
坐标轴上切分，将平面分为两个子区域。
生成之后，左子结点的$$x^{(1)}$$小于切分点的$$x^{(1)}$$,右子结点的$$x^{(1)}$$大于切分点的$$x^{(1)}$$。

（二）重复该过程，假设当前点的深度为j,那么选择$$l=(j\%k)+1$$,即$$x^{(l)}$$的中位数为切分点，
垂直$$x^{(l)}$$轴进行切分。

（三）直到没有实例可以切分，此时KD树划分完成

###2.搜索

（一）从根节点出发，如果x的当前小于切分点的坐标，则递归到左子结点，否则递归到右子结点。知道为叶结点为止。
（二）已此叶结点为“当前最近点”，不断回退。
（三）用当前节点来更新“当前最近点”。
（四）“当前最近点”一定是当前节点一个子节点对应的区域，检查该子节点对于的父节点的另一区域是否有更近的点，即，检查另一子节点对应的区域是否与以目标点为球心，“当前最近点”为半径的超球体相交。若相交，搜索另一区域，否则继续回退。（即如果当前划分域为$$x^{(l)}$$，并且该域中位数的点为$$x_k$$,那么即判断该超球体是否与$$x^{(l)}=x_k^{(l)}$$相交）
（五）回退到根节点，结束搜索。
	
###3.实现

见我另一篇博客[kdt](http://kdqzzxxcc.github.io/2014/11/03/KD_Tree%E5%AD%A6%E4%B9%A0.html)